{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Starting Definitions\n",
    "\n",
    "### Sample Space\n",
    "Suppose we have an experiment whose outcome depends on chance. We represent the outcome of the experiment by a Roman capital letter, such as $X$, called a <i>random variable</i>. The <i>sample space</i> of the experiment is the set of all possible outcomes. If the sample space is either finite or countably infinite, the random variable is said to be <i>discrete</i>. [1]\n",
    "\n",
    "### Distribution Function\n",
    "Let $X$ be a random variable which denotes the value of the outcome of a certain experiment, and assume that this experiment has only finitely many possible outcomes. Let $\\Omega$ be the sample space of the experiment. A <i>distribution function</i> for $X$ is a real-valued function $m$ whose domain is $\\Omega$ and which satisfies:\n",
    "<ol>\n",
    "    <li>$m(\\omega) \\ge 0$, for all $\\omega \\in \\Omega$, and</li>\n",
    "    <li>$\\sum_{\\omega \\in \\Omega}m(\\omega)=1$.\n",
    "</ol>\n",
    "\n",
    "For any subset $E$ of $\\Omega$, we define the <i>probability</i> of $E$ to be the number $P(E)$ given by\n",
    "\n",
    "$$P(E)=\\sum_{\\omega \\in E}m(\\omega).\\ [1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Probability\n",
    "\n",
    "Suppose we assign a distribution function $m$ to a sample space $\\Omega$ and learn that some event $E$ has occurred. How should we change the probabilities for the remaining events in $\\Omega$? Consider one such remaining event $F$, we shall call the new probability for $F$ the $\\it{conditional\\ probability\\ of}$ $F$ $\\it{given}$ $E$, and denote it by $P(F|E)$. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Consider an experiment that consists of rolling a die one time. Let $X$ be the outcome for this experiment. Consider the events $E$ and $F$ as $\\{X>4\\}$ and $\\{X=6\\}$, respectively. Assign $m(\\omega)=\\frac{1}{6}$ for $\\omega=\\{1,2,\\ldots,6\\}$. So $P(F)=\\frac{1}{6}$. Say that after rolling the die we are told that the event $E$ occurred, this leaves 5 and 6 as the only possibilities. In the abscence of further information we regard them as being equally likely, so the probability for $F$ becomes $\\frac{1}{2}$, making $P(F|E)=\\frac{1}{2}$. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Deeper Look\n",
    "\n",
    "We saw in the previous example that $P(F|E)=\\frac{1}{2}$. But how would one calculate that more mathematically? We see that the probability for any single outcome for the die is $\\frac{1}{6}$. So we can deduce that $P(F)=P(\\{X=6\\})=\\frac{1}{6}$ and $P(E)=P(\\{X>4\\})=\\frac{2}{6}$. One might notice that \n",
    "\n",
    "$$\\frac{P(F)}{P(E)}=\\frac{1/6}{2/6}=\\frac{1}{2}=P(F|E).$$\n",
    "\n",
    "Which is the same answer we arrived at in the previous example, but what happens if we set some event $G$ to $\\{X=1\\}$? We will observe that the above expression breaks down. Since $E$ occurred, that means that 5 and 6 are our only possible outcomes, since 1 is not either of those values, the probability $P(G|E)$ should be zero. However we will get the exact same result as above. How can we fix this? It's pretty clear that it should work if $G \\subseteq E$, but breaks down in other cases. The solution is to use the probability of the intersection between $E$ and $G$, $P(E \\cap G)$. Trying again we get\n",
    "\n",
    "$$\\frac{P(E \\cap G)}{P(E)}=\\frac{P(\\emptyset)}{P(E)}=\\frac{0}{2/6}=0=P(G|E)$$\n",
    "\n",
    "which is the correct solution in this case. So (without proof) $P(F|E)=\\frac{P(E \\cap F)}{P(E)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' Formula\n",
    "\n",
    "Suppose we have a set of events $H_1, H_2, \\ldots, H_m$ that are pairwise disjoint such that $\\Omega=\\bigcup_{i=1}^m H_i$. We call each of these events $\\it{hypotheses}$. We also have a an event $E$ that gives us information about which one of the hypotheses is the correct one. We call $E$ $\\it{evidence}$. Before we even obtain the evidence $E$, we have a set of $\\it{prior\\ probabilities}$ $\\{P(H_i)\\ |\\ i=\\{1,2,\\ldots,m\\}\\}$ for the hypotheses. Supposing we know which one of the hypotheses is the correct one, we can deduce the probability for $E$. Essentially, we know $P(E|H_i)$ for each $i$. We wish to find the probability for a hypothesis given some evidence or $P(H_i|E)$. These are known as $\\it{posterior\\ probabilities}$. To find the posterior probabilities, we write them in the following form:\n",
    "\n",
    "$$P(H_i|E) = \\frac{P(E \\cap H_i)}{P(E)}$$\n",
    "\n",
    "We can further calculate the numerator as $P(E|H_i)P(H_i)$, yielding\n",
    "\n",
    "$$P(H_i|E) = \\frac{P(E|H_i)P(H_i)}{P(E)}$$\n",
    "\n",
    "As only one of the $m$ hypotheses can occur, we can rewrite $P(E)$ as\n",
    "\n",
    "$$P(E)=P(E \\cap H_1) + P(E \\cap H_2) + \\ldots + P(E \\cap H_m)$$\n",
    "\n",
    "Which can in turn be rewritten as\n",
    "\n",
    "$$P(E|H_1)P(H_1) + P(E|H_2)P(H_2) + \\ldots + P(E|H_m)P(H_m)$$\n",
    "\n",
    "Putting it all together, we get the expression known as $\\it{Bayes'\\ Formula}$:\n",
    "\n",
    "$$P(H_i | E) = \\frac{P(E | H_i)P(H_i)}{\\sum_{k=1}^m P(E | H_k)P(H_k)}\\ [1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes'\n",
    "\n",
    "Naive Bayes models are a group of simple classification algorithms that are fast and often well-suited for high-dimensional datasets. They are often used to establish baselines for classification problems due to their speed and having few tunable parameters. The \"naivety\" of Naive Bayes comes from the fact that it makes use of simplifying assumptions to make training easier. One such assumption is mutual independence.\n",
    "\n",
    "### Mutual Independence\n",
    "\n",
    "The random variables $X_1, X_2, \\ldots, X_n$ are $\\it{mutually\\ independent}$ if\n",
    "$$P(X_1=r_1, X_2=r_2, \\ldots, X_n=r_n)=P(X_1=r_1)P(X_2=r_2)\\ldots P(X_n=r_n)$$\n",
    "\n",
    "for any choice of $r_i$. [1] In other words, we can just take the product of the individual probabilities.\n",
    "\n",
    "This assumption is useful for multinomial Naive Bayes, where features are represented as counts or count rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Example by Hand\n",
    "\n",
    "Say we have a dataset consisting of 500 dogs, 500 parrots, and 500 fish. We keep track of each animal's ability to swim, and whether or not it has wings, is green, or has teeth. Our data may look something like this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    Animal & Swim & Wings & Green & Teeth \\\\\n",
    "    \\hline\n",
    "    Dog & \\frac{450}{500} & \\frac{0}{500} & \\frac{0}{500} & \\frac{500}{500} \\\\\n",
    "    Parrot & \\frac{50}{500} & \\frac{500}{500} & \\frac{400}{500} & \\frac{0}{500} \\\\\n",
    "    Fish & \\frac{500}{500} & \\frac{0}{500} & \\frac{100}{500} & \\frac{50}{500}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now suppose we get a new data point with the following attributes:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    Swim & Wings & Green & Teeth \\\\\n",
    "    \\hline\n",
    "    True & False & True & False\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We want to figure out whether this new point should be classified as a dog, parrot, or fish based on its ability to swim, and its green coloring. Using what we have learned earlier, we can calculate the probability that this creature is a dog in the following manner:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    P(Dog\\ |\\ Swim,\\ Green) & = & \\frac{P(Swim\\ |\\ Dog) P(Green\\ |\\ Dog)P(Dog)}{P(Swim,\\ Green)}\\ by\\ mutual\\ independence\\\\\n",
    "    & = & \\frac{(450/500)(0/500)(1/3)}{P(Swim,\\ Green)} \\\\\n",
    "    & = & 0\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We simply repeat this calculation for each other option\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    P(Parrot\\ |\\ Swim,\\ Green) & = & \\frac{P(Swim\\ |\\ Parrot) P(Green\\ |\\ Parrot)P(Parrot)}{P(Swim,\\ Green)} \\\\\n",
    "    & = & \\frac{(50/500)(400/500)(1/3)}{P(Swim,\\ Green)} \\\\\n",
    "    & \\approx & \\frac{0.0264}{P(Swim,\\ Green)}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    P(Fish\\ |\\ Swim,\\ Green) & = & \\frac{P(Swim\\ |\\ Fish) P(Green\\ |\\ Fish)P(Fish)}{P(Swim,\\ Green)} \\\\\n",
    "    & = & \\frac{(500/500)(100/500)(1/3)}{P(Swim,\\ Green)} \\\\\n",
    "    & \\approx & \\frac{0.0666}{P(Swim,\\ Green)}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Since $P(Swim,\\ Green)$ is the same in each calculation, it acts as a simple scaling factor and can be ignored. The largest value remains the largest value when every other value is scaled by the same amount. So are final results are\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    Dog & Parrot & Fish \\\\\n",
    "    \\hline\n",
    "    0 & 0.0264 & 0.0666\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We see that the probability of this swimming green creature being a fish is the most likely choice, so we classify this particular animal as a fish. [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Example\n",
    "\n",
    "Naive Bayes is often used for text classification, where the word counts or frequencies within documents are used as features. We will demonstrate how Naive Bayes classifies these short documents from scikit learn's 20 Newsgroups corpus. [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups(data_home='datasets/')\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only use a few of these categories, below is an example of how the these documents look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['rec.autos', 'sci.space', 'rec.sport.baseball', 'sci.electronics', 'talk.politics.guns']\n",
    "\n",
    "train = fetch_20newsgroups(data_home='datasets', subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(data_home='datasets', subset='test', categories=categories)\n",
    "\n",
    "print(train.data[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this textual data for machine learning, we need a way to convert the contents into numerical vectors. We will use scikit learn's TF-IDF Vectorizer to accomplish this task. After that, all it takes is to simply fit the model to the training data and we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(train.data, train.target)\n",
    "labels = model.predict(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model fitted to our data, we can look at its results in a confusion matrix comparing the true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrix = confusion_matrix(test.target, labels)\n",
    "sns.heatmap(matrix.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it has a little bit of trouble separating autos, space, and guns from electronics, but considering electronics can be applied to each of those, it is not unreasonable that there is some confusion (pun not intended) there. Now we can use this model to classify any body of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('how massive are black holes?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('I fried my PCB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('Will HK produce the PSG1 again?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to Use Naive Bayes\n",
    "\n",
    "Naive Bayes models generally do not perform as well as more complicated models due to the strong assumptions that it makes about the data. However, Naive Bayes does have several advantages:\n",
    "\n",
    "<ul>\n",
    "    <li>Fast in both training and prediction</li>\n",
    "    <li>Easily interpretable, straightforward probabilistic predictions</li>\n",
    "    <li>Few tunable parameters</li>\n",
    "</ul>\n",
    "\n",
    "Should Naive Bayes perform well for your task, then you have a very fast and interpretable solution for your problem. If not, then at least you have a good initial baseline for you classification task as you begin to explore more complex models.\n",
    "\n",
    "Naive Bayes perform especially well under any of the following conditions:\n",
    "\n",
    "<ul>\n",
    "    <li>For very well separated categories, so model complexity is not as important</li>\n",
    "    <li>For very high-dimensional data, when model complexity is less important</li>\n",
    "    <li>When the naive assumptions about your data are true, which is very rare in practice [3]</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] C. Grinstead and J. Snell. $\\it{Introduction\\ to\\ Probability}$, 1997. \n",
    "\n",
    "[2] R. Saxena. How the Naive Bayes Classifier Works in Machine Learning http://dataaspirant.com/2017/02/06/naive-bayes-classifier-machine-learning/ 2017.\n",
    "\n",
    "[3] J. VanderPlas. In Depth: Naive Bayes Classification https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html, 2016.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
